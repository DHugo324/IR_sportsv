import os
import json
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer, util
import torch

# 1. è¼‰å…¥ä¸­æ–‡ SBERT æ¨¡å‹
model = SentenceTransformer('shibing624/text2vec-base-chinese')

# 2. è®€å–æ‰€æœ‰æ–‡ç« ï¼ˆåŸå§‹è³‡æ–™ï¼‰
folder_path = "articles/predicted_articles"
articles = {}
corpus_embeddings = []
article_ids = []
article_dates = {}  # å„²å­˜æ¯ç¯‡æ–‡ç« çš„æ—¥æœŸ

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        article_id = data["id"]
        title = data["title"]
        tags = " ".join(data.get("tags", []))
        body = "".join(data["article-content"])
        date_str = data["date"]
        date_obj = datetime.strptime(date_str, "%Y/%m/%d")
        article_dates[article_id] = date_obj

        # å„è‡ªè¨ˆç®— embedding å†å¹³å‡ï¼ˆæ¨™é¡Œã€tagã€å…§æ–‡ï¼‰
        title_emb = model.encode(title, convert_to_tensor=True, normalize_embeddings=True)
        tag_emb = model.encode(tags, convert_to_tensor=True, normalize_embeddings=True)
        body_emb = model.encode(body, convert_to_tensor=True, normalize_embeddings=True)
        combined_emb = (title_emb + tag_emb + body_emb) / 3

        articles[article_id] = title + "ã€‚" + tags + "ã€‚" + body
        corpus_embeddings.append(combined_emb)
        article_ids.append(article_id)

# 3. æŒ‡å®šæŸ¥è©¢æ–‡ç«  ID
target_id = "118870"
target_title = ""
target_tags = ""
target_body = ""
target_date = None

for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if data["id"] == target_id:
            target_title = data["title"]
            target_tags = " ".join(data.get("tags", []))
            target_body = "".join(data["article-content"])
            target_date = datetime.strptime(data["date"], "%Y/%m/%d")
            break

# 4. å»ºç«‹æŸ¥è©¢æ–‡ç«  embeddingï¼ˆæ¨™é¡Œ + tag + å…§æ–‡å¹³å‡ï¼‰
target_title_emb = model.encode(target_title, convert_to_tensor=True, normalize_embeddings=True)
target_tag_emb = model.encode(target_tags, convert_to_tensor=True, normalize_embeddings=True)
target_body_emb = model.encode(target_body, convert_to_tensor=True, normalize_embeddings=True)
target_embedding = (target_title_emb + target_tag_emb + target_body_emb) / 3

# 5. éæ¿¾æ—¥æœŸ Â±10 å¤©å…§çš„æ–‡ç« 
start_date = target_date - timedelta(days=10)
end_date = target_date + timedelta(days=10)

filtered_embeddings = []
filtered_ids = []

for i, aid in enumerate(article_ids):
    if aid == target_id:
        continue
    article_date = article_dates[aid]
    if start_date <= article_date <= end_date:
        filtered_embeddings.append(corpus_embeddings[i])
        filtered_ids.append(aid)

# 6. è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰
filtered_tensor = torch.stack(filtered_embeddings)
cos_scores = util.cos_sim(target_embedding, filtered_tensor)[0]

# 7. æ‰¾å‡ºå‰3ç¯‡æœ€ç›¸ä¼¼
top_results = sorted(list(enumerate(cos_scores)), key=lambda x: x[1], reverse=True)

print(f"ğŸ” å’Œæ–‡ç«  {target_id} æœ€ç›¸ä¼¼çš„3ç¯‡æ–‡ç« ï¼ˆé™å®š Â±10 å¤©å…§ï¼‰ï¼š\n")
count = 0
for idx, score in top_results:
    similar_id = filtered_ids[idx]
    print(f"ğŸ“„ æ–‡ç«  IDï¼š{similar_id}, ç›¸ä¼¼åº¦ï¼š{score:.4f}")
    print(f"â¡ï¸ æ–‡ç« æ¨™é¡Œï¼š{articles[similar_id][:50]}...\n")
    count += 1
    if count == 3:
        break